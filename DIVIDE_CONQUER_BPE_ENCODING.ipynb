{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7aa557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 20:37:42.870595: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-03 20:37:42.904299: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import tiktoken\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b25e1799",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f772ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Byte Pair Encoder\n",
    "enc = tiktoken.get_encoding(\"p50k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e2f9528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on file : republic\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for file in os.listdir(\"./data\"):\n",
    "    print(f\"Working on file : {file}\")\n",
    "    with open(f\"./data/{file}\", \"r\") as f:\n",
    "        data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ccfbe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate\n",
    "data = \"\\n\".join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "542d5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = re.sub(\"\\n+\", \"\\n\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75860fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I went down yesterday to the Piraeus with Glaucon the son of Ariston, that I might offer up my prayers to the goddess (Bendis, the Thracian Artemis.); and also because I wanted to see in what manner they would celebrate the festival, which was a new thing. I was delighted with the procession of the inhabitants; but that of the Thracians was equally, if not more, beautiful. When we had finished our prayers and viewed the spectacle, we turned in the direction of the city; and at that instant Polem'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c5e5b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-03 20:37:53.713517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-03 20:37:53.732175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-03 20:37:53.732314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-03 20:37:53.732823: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-03 20:37:53.735085: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-03 20:37:53.735190: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-03 20:37:53.735240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-03 20:37:54.027785: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-03 20:37:54.027920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-03 20:37:54.027973: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-03 20:37:54.028199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 639 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "data = tf.convert_to_tensor(enc.encode(data), tf.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "781d7a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([147085])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6372b2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.stack([data[idx:idx+block_size] for idx in range(len(data)-block_size)])\n",
    "y = tf.stack([data[idx+1:idx+block_size+1] for idx in range(len(data)-block_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea36fa36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([146957, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5c0e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices((x, y))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67c16131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146957"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec853e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(dataset, train_split, val_split, test_split):\n",
    "    dataset_size = len(dataset)\n",
    "    dataset = dataset.shuffle(dataset_size)\n",
    "    train_size = int(train_split * dataset_size)\n",
    "    val_size = int(val_split * dataset_size)\n",
    "    \n",
    "    train_data = dataset.take(train_size)\n",
    "    val_data = dataset.skip(train_size).take(val_size)\n",
    "    test_data = dataset.skip(train_size+val_size)\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecb68fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = train_val_test_split(data, 0.7, 0.2, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87a0199a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25717"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the train:\n",
    "# We split it into 4 shards\n",
    "offset = len(train) // 4\n",
    "offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1082a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_list = []\n",
    "for i in range(4):\n",
    "    if i == 3:\n",
    "        shard_list.append(train.skip(offset * i).shuffle(offset).batch(batch_size))\n",
    "    else:\n",
    "        shard_list.append(train.skip(offset * i).take(offset).shuffle(offset).batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3924bd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shard_list[0]) == len(shard_list[1]) == len(shard_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82a81bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shard_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba6ae545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<BatchDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.float16, name=None), TensorSpec(shape=(None, 128), dtype=tf.float16, name=None))>,\n",
       " <BatchDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.float16, name=None), TensorSpec(shape=(None, 128), dtype=tf.float16, name=None))>,\n",
       " <BatchDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.float16, name=None), TensorSpec(shape=(None, 128), dtype=tf.float16, name=None))>,\n",
       " <BatchDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.float16, name=None), TensorSpec(shape=(None, 128), dtype=tf.float16, name=None))>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shard_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15dc8f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, ds in enumerate(shard_list):\n",
    "    ds.save(f\"./shards/shard_{ind}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ebe174f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = val.batch(batch_size)\n",
    "val.save(\"./shards/val_shard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "085a5122",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.batch(batch_size)\n",
    "test.save(\"./shards/test_shard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d423ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./shards\"\n",
    "shards = []\n",
    "for file in os.listdir(path):\n",
    "    if file.startswith(\"shard\"):\n",
    "        shards.append(tf.data.Dataset.load(f\"{path}/{file}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e92f4ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_LoadDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.float16, name=None), TensorSpec(shape=(None, 128), dtype=tf.float16, name=None))>,\n",
       " <_LoadDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.float16, name=None), TensorSpec(shape=(None, 128), dtype=tf.float16, name=None))>,\n",
       " <_LoadDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.float16, name=None), TensorSpec(shape=(None, 128), dtype=tf.float16, name=None))>,\n",
       " <_LoadDataset element_spec=(TensorSpec(shape=(None, 128), dtype=tf.float16, name=None), TensorSpec(shape=(None, 128), dtype=tf.float16, name=None))>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0035ef8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 128), dtype=float16, numpy=\n",
       " array([[1.980e+02, 2.436e+03, 5.230e+02, ..., 6.120e+02, 3.180e+02,\n",
       "         2.570e+02],\n",
       "        [1.980e+02, 3.600e+04, 4.070e+02, ..., 3.180e+02, 4.368e+03,\n",
       "         1.100e+01],\n",
       "        [7.740e+03, 8.200e+01, 3.290e+02, ..., 2.900e+02, 3.510e+02,\n",
       "         5.840e+02],\n",
       "        ...,\n",
       "        [3.390e+02, 5.310e+02, 1.300e+01, ..., 2.900e+02, 3.290e+02,\n",
       "         2.620e+02],\n",
       "        [3.790e+02, 1.752e+03, 1.100e+01, ..., 2.860e+02, 5.970e+02,\n",
       "         3.296e+03],\n",
       "        [3.560e+02, 4.070e+02, 6.350e+02, ..., 5.340e+02, 1.103e+03,\n",
       "         4.460e+03]], dtype=float16)>,\n",
       " <tf.Tensor: shape=(64, 128), dtype=float16, numpy=\n",
       " array([[2.4360e+03, 5.2300e+02, 3.0000e+01, ..., 3.1800e+02, 2.5700e+02,\n",
       "         1.3650e+03],\n",
       "        [3.6000e+04, 4.0700e+02, 1.3000e+01, ..., 4.3680e+03, 1.1000e+01,\n",
       "         3.1800e+03],\n",
       "        [8.2000e+01, 3.2900e+02, 2.6200e+02, ..., 3.5100e+02, 5.8400e+02,\n",
       "         1.8290e+03],\n",
       "        ...,\n",
       "        [5.3100e+02, 1.3000e+01, 1.9800e+02, ..., 3.2900e+02, 2.6200e+02,\n",
       "         1.6216e+04],\n",
       "        [1.7520e+03, 1.1000e+01, 2.9000e+02, ..., 5.9700e+02, 3.2960e+03,\n",
       "         2.8600e+02],\n",
       "        [4.0700e+02, 6.3500e+02, 2.2820e+03, ..., 1.1030e+03, 4.4600e+03,\n",
       "         1.3000e+01]], dtype=float16)>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(shards[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3058930e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shards[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a71da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
